---
title: "STAT/MATH 495: Problem Set 06"
author: "Harrison Marick"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
```





# Collaboration

Please indicate who you collaborated with on this assignment: 

I ride solo. 


# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation

```{r}
n_sample=100
pred1<-rep(0, n_sample) #vector for 1st model
pred2<-rep(0, n_sample) #vector for 2nd model
for (i in 1:n_sample){
  samp<-generate_sample(f, n, sigma) #create sample
  m1<-smooth.spline(samp$x, samp$y, df=2) #fit model1
  m2<-smooth.spline(samp$x, samp$y, df=99) #fit model2
  pred1[i]<-predict(m1, 0.95)$y #store predicted
  pred2[i]<-predict(m2, 0.95)$y 
}


mse1=mean((pred1-f(0.95))^2)
mse2=mean((pred2-f(0.95))^2)
var1=var(pred1)
var2=var(pred2)
bias1=mean(pred1)-f(0.95)
bias2=mean(pred2)-f(0.95)
```


# Tables

As done in Lec 2.7, for both

* An `lm` regression AKA a `smooth.splines(x, y, df=2)` model fit 
* A `smooth.splines(x, y, df=99)` model fit 

output tables comparing:

|  MSE| bias_squared|   var| irreducible|   sum|
|----:|------------:|-----:|-----------:|-----:|
|     X|           X  |     X |      X |         X |

where `sum = bias_squared + var + irreducible`. You can created cleanly formatted tables like the one above by piping a data frame into `knitr::kable(digits=4)`.

```{r}
mse=c(mse1, mse2)
var=c(var1, var2)
bias_squared=c(bias1^2, bias2^2)
irreducible=c(sigma^2, sigma^2)
sum=bias_squared+var+irreducible
dat=data.frame(df=c(2, 99), MSE=mse, Var=var, bias_squared=bias_squared, irreducible=irreducible, sum=sum)
knitr::kable(dat, digits=4)
```


# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
1. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
1. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. The easiest sanity check would be to look at the table I created above. The bias for the first model should be much higher than the second model, and the variance should be much smaller for the first than the second. This appears to be the case. The second sanity check I can think of would be to plot the histogram of the predicted values for each model. In the plots, if we see tremendous variation in the fitted value (really flat histogram) for $x=0.95$ when $df=99$ and very little variation in the fitted value for $x=0.95$, then we are likely on the right track.

2. For each sample in the for-loop, predict values for all $x$ and calculate the MSE of each sample, which will be stored in a vector. The mean of this vector is our MSE for all values of $x$.

3. The bias for the model with $df=2$ is -0.11, which is rather significant given the expected value is just 0.9025. Having said that, with a variance of just 0.00065, this model is consistent, and it's predictions will not change much as datasets change. Overall, with an MSE of just 0.013 compared to 0.019 for the model with $df=99$, I feel more comfortable that this model will perform better out of sample. With a lot of variation in the predicted values, the model where $df=99$ is less reliable. Having said that, I would likely use a different df parameter if given the chance. 
